{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syst√®me Multi-Agents avec Q-Learning pour l'Ordonnancement des Patients\n",
    "\n",
    "Ce notebook d√©montre l'impl√©mentation d'un syst√®me multi-agents pour l'optimisation de l'ordonnancement des patients dans un environnement de soins.\n",
    "\n",
    "## Caract√©ristiques principales:\n",
    "- **Donn√©es Strictes**: Bas√©es sur la table de comp√©tences r√©elle (Image WhatsApp).\n",
    "- **M√©taheuristiques hybrides**: Algorithme G√©n√©tique, Recherche Tabou, Recuit Simul√©\n",
    "- **Modes de collaboration**: Amis (partage complet) et Ennemis (comp√©tition)\n",
    "- **Auto-adaptation**: Q-Learning pour la s√©lection des voisinages (Insertion vs Swap)\n",
    "\n",
    "Bas√© sur le diaporama: *\"Optimisation collaborative : Agents auto-adaptatifs, Apprentissage par renforcement\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration et imports\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ajout du chemin racine\n",
    "sys.path.insert(0, os.path.abspath(os.getcwd()))\n",
    "\n",
    "# Seed pour reproductibilit√©\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Imports du projet remasteris√©\n",
    "from core.environment import create_default_environment\n",
    "from core.neighborhoods import NeighborhoodManager\n",
    "from core.shared_memory import SharedMemoryPool, Solution\n",
    "from core.agents import MultiAgentSystem, CollaborationMode\n",
    "from visualization import plot_gantt\n",
    "\n",
    "print(\"‚úì Tous les modules sont charg√©s et pr√™ts!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environnement d'Ordonnancement (Strict)\n",
    "\n",
    "L'environnement mod√©lise l'ordonnancement des patients selon les contraintes strictes de l'image fournie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er l'environnement\n",
    "env = create_default_environment()\n",
    "\n",
    "print(f\"Configuration de l'environnement:\")\n",
    "print(f\"  - Nombre de patients: {env.num_patients}\")\n",
    "print(f\"  - Comp√©tences: {env.skills}\")\n",
    "print(f\"  - Nombre total de t√¢ches: {len(env.all_tasks)}\")\n",
    "\n",
    "# Solution initiale (Al√©atoire)\n",
    "initial_solution = env.build_initial_solution(random_order=True)\n",
    "initial_makespan, initial_times, _ = env.evaluate(initial_solution, return_schedule=True)\n",
    "print(f\"\\nMakespan initial (Al√©atoire): {initial_makespan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser le planning initial\n",
    "if initial_times:\n",
    "    plot_gantt(\n",
    "        initial_times, env.skills, env.num_patients,\n",
    "        title=f\"Planning Initial (Cmax = {initial_makespan})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fonctions de Voisinage & Q-Learning\n",
    "\n",
    "Pour respecter les contraintes strictes (1 comp√©tence sp√©cifique par t√¢che), nous utilisons uniquement les voisinages valides :\n",
    "- **C**: Insertion (D√©placer une t√¢che dans la file du m√©decin)\n",
    "- **E**: Swap (√âchanger deux t√¢ches chez le m√™me m√©decin)\n",
    "\n",
    "Le **Q-Learning** va apprendre lequel des deux est le plus efficace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©monstration Q-Learning via un Agent Tabou\n",
    "mas_demo = MultiAgentSystem(env, mode=CollaborationMode.FRIENDS, use_qlearning=True)\n",
    "agent_tabu = mas_demo.add_agent('tabu', 'Demo_Agent', tabu_tenure=5)\n",
    "\n",
    "# On force quelques mises √† jour pour la d√©mo\n",
    "if agent_tabu.q_selector:\n",
    "    print(\"Simulation de l'apprentissage...\")\n",
    "    # Simulation : L'insertion (C) donne de meilleurs r√©sultats ici\n",
    "    for _ in range(10):\n",
    "        agent_tabu.q_selector.update_with_result('C', 100) # Bonne r√©compense\n",
    "        agent_tabu.q_selector.update_with_result('E', 110) # Moins bonne r√©compense (Cmax plus haut)\n",
    "    \n",
    "    stats = agent_tabu.q_selector.get_statistics()\n",
    "    q_table = agent_tabu.q_selector.get_q_table()\n",
    "    \n",
    "    print(\"\\n√âtat du Q-Learning :\")\n",
    "    print(f\"  Voisinages actifs : {list(stats.keys())}\")\n",
    "    print(f\"  Table Q : {q_table}\")\n",
    "else:\n",
    "    print(\"Q-Learning d√©sactiv√©.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Syst√®me Multi-Agents - Mode AMIS (Collaboratif)\n",
    "\n",
    "En mode Amis, les agents (AG, Tabou, RS) partagent leurs solutions via un **Espace M√©moire Partag√© (EMP)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Lancement SMA Collaboratif (100 it√©rations)...\")\n",
    "\n",
    "mas_friends = MultiAgentSystem(env, mode=CollaborationMode.FRIENDS, use_qlearning=True)\n",
    "\n",
    "# Trio complet\n",
    "mas_friends.add_agent('genetic', 'AG_Main', population_size=15)\n",
    "mas_friends.add_agent('tabu', 'Tabu_Main', tabu_tenure=10)\n",
    "mas_friends.add_agent('sa', 'RS_Main', initial_temp=100)\n",
    "\n",
    "best_sol_friends = mas_friends.run(n_iterations=100, verbose=True)\n",
    "\n",
    "print(f\"\\nüèÜ Meilleur Makespan (AMIS): {best_sol_friends.fitness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation du r√©sultat final Collaboratif\n",
    "if best_sol_friends:\n",
    "    cmax, times, _ = env.evaluate(best_sol_friends.sequences, return_schedule=True)\n",
    "    plot_gantt(\n",
    "        times, env.skills, env.num_patients,\n",
    "        title=f\"R√©sultat SMA Amis (Cmax={cmax})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Syst√®me Multi-Agents - Mode ENNEMIS (Comp√©titif)\n",
    "\n",
    "En mode Ennemis, les agents travaillent isol√©ment et ne partagent pas leur m√©moire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öîÔ∏è Lancement SMA Comp√©titif (100 it√©rations)...\")\n",
    "\n",
    "mas_enemies = MultiAgentSystem(env, mode=CollaborationMode.ENEMIES, use_qlearning=True)\n",
    "\n",
    "mas_enemies.add_agent('genetic', 'AG_Enemy')\n",
    "mas_enemies.add_agent('tabu', 'Tabu_Enemy')\n",
    "mas_enemies.add_agent('sa', 'RS_Enemy')\n",
    "\n",
    "best_sol_enemies = mas_enemies.run(n_iterations=100, verbose=True)\n",
    "\n",
    "print(f\"\\nüèÜ Meilleur Makespan (ENNEMIS): {best_sol_enemies.fitness}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparaison et G√©n√©ration des Tableaux (PDF)\n",
    "\n",
    "G√©n√©ration des m√©triques pour les slides 25 et 26."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"        R√âSULTATS POUR LE RAPPORT (PDF)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# R√©sultats simul√©s sur la derni√®re ex√©cution\n",
    "score_amis = best_sol_friends.fitness\n",
    "score_ennemis = best_sol_enemies.fitness\n",
    "\n",
    "print(f\"{'Configuration':<25} | {'Makespan (Score)'}\")\n",
    "print(\"-\"*45)\n",
    "print(f\"{'Mode AMIS (Collaboratif)':<25} | {score_amis}\")\n",
    "print(f\"{'Mode ENNEMIS (Comp√©titif)':<25} | {score_ennemis}\")\n",
    "\n",
    "gain = ((score_ennemis - score_amis) / score_ennemis) * 100\n",
    "print(\"-\"*45)\n",
    "if gain > 0:\n",
    "    print(f\"‚úÖ La collaboration a am√©lior√© le score de {gain:.2f}%\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Pas de gain significatif observ√© sur cette run courte.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Le syst√®me respecte les contraintes strictes. Le **mode Amis** coupl√© au **Q-Learning** (choix dynamique entre Insertion et Swap) offre g√©n√©ralement la meilleure convergence en √©vitant les minimums locaux gr√¢ce √† l'Espace M√©moire Partag√©."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
