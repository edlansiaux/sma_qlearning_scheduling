{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Système Multi-Agents avec Q-Learning pour l'Ordonnancement des Patients\n",
    "\n",
    "Ce notebook démontre l'implémentation d'un système multi-agents pour l'optimisation de l'ordonnancement des patients dans un environnement de soins.\n",
    "\n",
    "## Caractéristiques principales:\n",
    "- **Métaheuristiques hybrides**: Algorithme Génétique, Recherche Tabou, Recuit Simulé\n",
    "- **Modes de collaboration**: Amis (partage complet) et Ennemis (compétition)\n",
    "- **Auto-adaptation**: Q-Learning pour la sélection des voisinages\n",
    "- **Diversité**: Espace Mémoire Partagé (EMP) avec contrôle de distance\n",
    "- **5 fonctions de voisinage**: A, B, C, D, E (C et E actifs pour les contraintes strictes)\n",
    "\n",
    "Basé sur le diaporama: *\"Optimisation collaborative : Agents auto-adaptatifs, Apprentissage par renforcement\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration et imports\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ajout du chemin courant\n",
    "sys.path.insert(0, os.path.abspath(os.getcwd()))\n",
    "\n",
    "# Seed pour reproductibilité\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Imports du projet\n",
    "from core.environment import create_default_environment, Task\n",
    "from core.neighborhoods import NeighborhoodManager\n",
    "from core.qlearning import QLearningAgent\n",
    "from core.shared_memory import SharedMemoryPool, Solution, ElitePool\n",
    "from core.agents import (\n",
    "    GeneticAgent, TabuAgent, SimulatedAnnealingAgent,\n",
    "    MultiAgentSystem, CollaborationMode\n",
    ")\n",
    "from visualization import plot_gantt\n",
    "\n",
    "print(\"✓ Tous les modules importés avec succès!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environnement d'Ordonnancement\n",
    "\n",
    "L'environnement modélise l'ordonnancement des patients avec:\n",
    "- 10 patients\n",
    "- 5 opérations maximum par patient\n",
    "- 6 compétences/ressources médicales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer l'environnement\n",
    "env = create_default_environment()\n",
    "\n",
    "print(f\"Configuration de l'environnement:\")\n",
    "print(f\"  - Nombre de patients: {env.num_patients}\")\n",
    "print(f\"  - Compétences: {env.skills}\")\n",
    "print(f\"  - Opérations max par patient: {env.max_ops}\")\n",
    "print(f\"  - Nombre total de tâches: {len(env.all_tasks)}\")\n",
    "\n",
    "# Solution initiale\n",
    "initial_solution = env.build_initial_solution(random_order=False)\n",
    "initial_makespan, initial_times, _ = env.evaluate(initial_solution, return_schedule=True)\n",
    "print(f\"\\nMakespan initial (ordre naïf): {initial_makespan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser le planning initial\n",
    "if initial_times:\n",
    "    plot_gantt(\n",
    "        initial_times, env.skills, env.num_patients,\n",
    "        title=f\"Planning Initial (Cmax = {initial_makespan})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fonctions de Voisinage\n",
    "\n",
    "Les voisinages actifs pour ce problème strict sont principalement :\n",
    "- **C**: Insertion dans le même planning (MIS)\n",
    "- **E**: Échange dans le même planning (SSMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Démonstration des voisinages\n",
    "nm = NeighborhoodManager()\n",
    "\n",
    "print(\"Fonctions de voisinage disponibles:\")\n",
    "for name, func in nm.neighborhoods.items():\n",
    "    print(f\"  {name}: {func.name}\")\n",
    "\n",
    "# Générer des voisins pour chaque fonction valide\n",
    "test_solution = env.build_initial_solution(random_order=True)\n",
    "test_makespan, _, _ = env.evaluate(test_solution)\n",
    "print(f\"\\nSolution de test: Makespan = {test_makespan}\")\n",
    "\n",
    "print(\"\\nGénération de voisins (sur voisinages actifs C & E):\")\n",
    "for name in ['C', 'E']:\n",
    "    neighbor = nm.generate_neighbor(test_solution, name, env.skills, env.max_ops)\n",
    "    if neighbor:\n",
    "        neighbor_makespan, _, _ = env.evaluate(neighbor)\n",
    "        diff = neighbor_makespan - test_makespan\n",
    "        print(f\"  Voisinage {name}: Makespan = {neighbor_makespan} (Δ = {diff:+d})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Q-Learning pour l'Auto-Adaptation\n",
    "\n",
    "Le Q-Learning permet de sélectionner automatiquement la meilleure fonction de voisinage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un agent Q-Learning\n",
    "q_agent = QLearningAgent(\n",
    "    states=['C', 'E'],\n",
    "    alpha=0.15,    # Taux d'apprentissage\n",
    "    gamma=0.9,     # Facteur d'actualisation\n",
    "    epsilon=0.5,   # Exploration initiale\n",
    "    epsilon_decay=0.99\n",
    ")\n",
    "\n",
    "print(\"Paramètres Q-Learning:\")\n",
    "print(f\"  α (learning rate): {q_agent.alpha}\")\n",
    "print(f\"  γ (discount factor): {q_agent.gamma}\")\n",
    "print(f\"  ε (exploration): {q_agent.epsilon}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation d'apprentissage simple\n",
    "reward_means = {'C': 1.0, 'E': 0.5}\n",
    "epsilon_history = []\n",
    "\n",
    "for episode in range(100):\n",
    "    action = q_agent.select_action()\n",
    "    reward = random.gauss(reward_means[action], 0.2)\n",
    "    q_agent.update(action, reward)\n",
    "    q_agent.decay_epsilon()\n",
    "    epsilon_history.append(q_agent.epsilon)\n",
    "\n",
    "# Visualiser la table Q (Affichage textuel)\n",
    "q_table = q_agent.get_q_table_formatted()\n",
    "print(\"\\nTable Q après apprentissage (Scores par transition):\")\n",
    "for s_from, actions in q_table.items():\n",
    "    print(f\"État {s_from} -> {actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Espace Mémoire Partagé (EMP) avec Diversité\n",
    "\n",
    "L'EMP stocke les bonnes solutions trouvées tout en maintenant leur diversité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer l'EMP\n",
    "emp = SharedMemoryPool(\n",
    "    max_size=15,\n",
    "    min_distance=2,      # R: distance minimale\n",
    "    diversity_threshold=0.4  # DT: seuil de diversité\n",
    ")\n",
    "\n",
    "print(\"Paramètres EMP:\")\n",
    "print(f\"  Taille max: {emp.max_size}\")\n",
    "print(f\"  Distance minimale (R): {emp.min_distance}\")\n",
    "\n",
    "# Remplir l'EMP avec quelques solutions aléatoires\n",
    "for i in range(20):\n",
    "    sol = env.build_initial_solution(random_order=True)\n",
    "    fitness, _, _ = env.evaluate(sol)\n",
    "    solution = Solution(sequences=sol, fitness=fitness, agent_id=f\"test_{i}\")\n",
    "    emp.insert(solution, iteration=i)\n",
    "\n",
    "stats = emp.get_statistics()\n",
    "print(f\"\\nStatistiques EMP:\")\n",
    "print(f\"  Solutions stockées: {stats['size']}\")\n",
    "print(f\"  Meilleure fitness: {stats['global_best_fitness']}\")\n",
    "print(f\"  Insertions: {stats['insertions']}\")\n",
    "print(f\"  Rejets (doublons): {stats['rejections_duplicate']}\")\n",
    "print(f\"  Rejets (diversité): {stats['rejections_diversity']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Système Multi-Agents - Mode AMIS\n",
    "\n",
    "En mode Amis, les agents partagent leurs solutions complètes via l'EMP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le système multi-agents en mode Amis\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "mas_friends = MultiAgentSystem(env, mode=CollaborationMode.FRIENDS, use_qlearning=True)\n",
    "\n",
    "# Ajouter des agents de différents types\n",
    "mas_friends.add_agent('genetic', 'AG_1', population_size=15)\n",
    "mas_friends.add_agent('tabu', 'Tabu_1', tabu_tenure=10)\n",
    "mas_friends.add_agent('sa', 'RS_1', initial_temp=100)\n",
    "\n",
    "print(f\"Mode: {mas_friends.mode}\")\n",
    "print(f\"Agents: {list(mas_friends.agents.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exécuter l'optimisation\n",
    "print(\"Optimisation collaborative en cours...\")\n",
    "best_solution_friends = mas_friends.run(n_iterations=50, verbose=True)\n",
    "\n",
    "stats_friends = mas_friends.get_statistics()\n",
    "print(f\"\\nMeilleur Makespan trouvé: {stats_friends['global_best_fitness']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser la convergence (Extraction manuelle de l'historique d'un agent pour l'exemple)\n",
    "history_friends = list(mas_friends.agents.values())[0].fitness_history\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history_friends, label=\"AG_1 History\")\n",
    "plt.title(\"Convergence (Exemple Agent AG_1 - Mode Amis)\")\n",
    "plt.xlabel(\"Itérations\")\n",
    "plt.ylabel(\"Makespan\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser le planning optimisé\n",
    "if best_solution_friends:\n",
    "    final_makespan, final_times, _ = env.evaluate(\n",
    "        best_solution_friends.sequences, return_schedule=True\n",
    "    )\n",
    "    \n",
    "    plot_gantt(\n",
    "        final_times, env.skills, env.num_patients,\n",
    "        title=f\"Résultat Optimisé (Mode Amis, Cmax={final_makespan})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Système Multi-Agents - Mode ENNEMIS\n",
    "\n",
    "En mode Ennemis, les agents ne partagent que leurs valeurs de fitness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le système en mode Ennemis\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "mas_enemies = MultiAgentSystem(env, mode=CollaborationMode.ENEMIES, use_qlearning=True)\n",
    "\n",
    "mas_enemies.add_agent('genetic', 'AG_Enemy')\n",
    "mas_enemies.add_agent('tabu', 'Tabu_Enemy')\n",
    "mas_enemies.add_agent('sa', 'RS_Enemy')\n",
    "\n",
    "print(f\"Mode: {mas_enemies.mode}\")\n",
    "print(\"\\nEn mode Ennemis, les agents sont en compétition.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exécuter l'optimisation\n",
    "print(\"\\nOptimisation compétitive en cours...\")\n",
    "best_solution_enemies = mas_enemies.run(n_iterations=50, verbose=True)\n",
    "\n",
    "stats_enemies = mas_enemies.get_statistics()\n",
    "print(f\"\\nMeilleur Makespan trouvé: {stats_enemies['global_best_fitness']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparaison Finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau de comparaison\n",
    "print(\"=\"*60)\n",
    "print(\"        COMPARAISON DES MODES DE COLLABORATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Métrique':<35} {'AMIS':>12} {'ENNEMIS':>12}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Meilleur Makespan':<35} {stats_friends['global_best_fitness']:>12} {stats_enemies['global_best_fitness']:>12}\")\n",
    "print(f\"{'Taille finale EMP':<35} {stats_friends['emp_stats']['size']:>12} {stats_enemies['emp_stats']['size']:>12}\")\n",
    "print(f\"{'Insertions EMP':<35} {stats_friends['emp_stats']['insertions']:>12} {stats_enemies['emp_stats']['insertions']:>12}\")\n",
    "\n",
    "improvement_friends = (initial_makespan - stats_friends['global_best_fitness']) / initial_makespan * 100\n",
    "improvement_enemies = (initial_makespan - stats_enemies['global_best_fitness']) / initial_makespan * 100\n",
    "print(f\"{'Amélioration vs initial (%)':<35} {improvement_friends:>11.1f}% {improvement_enemies:>11.1f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planning final optimisé\n",
    "if stats_friends['global_best_fitness'] <= stats_enemies['global_best_fitness']:\n",
    "    best_overall = best_solution_friends\n",
    "    best_mode = \"AMIS\"\n",
    "else:\n",
    "    best_overall = best_solution_enemies\n",
    "    best_mode = \"ENNEMIS\"\n",
    "\n",
    "print(f\"\\nMeilleure solution globale trouvée par le mode {best_mode}\")\n",
    "\n",
    "final_makespan, final_times, _ = env.evaluate(best_overall.sequences, return_schedule=True)\n",
    "\n",
    "plot_gantt(\n",
    "    final_times, env.skills, env.num_patients,\n",
    "    title=f\"Planning Optimisé Final (Mode {best_mode}, Cmax = {final_makespan})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Ce système multi-agents démontre comment combiner:\n",
    "\n",
    "1. **Plusieurs métaheuristiques** (AG, Tabou, Recuit) pour diversifier la recherche\n",
    "2. **Deux modes de collaboration** (Amis/Ennemis) pour différentes stratégies\n",
    "3. **L'auto-adaptation via Q-Learning** pour choisir intelligemment les voisinages\n",
    "4. **Le contrôle de diversité** dans l'EMP pour éviter la convergence prématurée"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
