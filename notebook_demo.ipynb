{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Système Multi-Agents avec Q-Learning pour l'Ordonnancement des Patients\n",
    "\n",
    "Ce notebook démontre l'implémentation d'un système multi-agents pour l'optimisation de l'ordonnancement des patients dans un environnement de soins.\n",
    "\n",
    "## Caractéristiques principales:\n",
    "- **Métaheuristiques hybrides**: Algorithme Génétique, Recherche Tabou, Recuit Simulé\n",
    "- **Modes de collaboration**: Amis (partage complet) et Ennemis (compétition)\n",
    "- **Auto-adaptation**: Q-Learning pour la sélection des voisinages\n",
    "- **Diversité**: Espace Mémoire Partagé (EMP) avec contrôle de distance\n",
    "- **5 fonctions de voisinage**: A, B, C, D, E\n",
    "\n",
    "Basé sur le diaporama: *\"Optimisation collaborative : Agents auto-adaptatifs, Apprentissage par renforcement\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration et imports\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seed pour reproductibilité\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Imports du projet\n",
    "from core.environment import SchedulingEnvironment, create_default_environment, Task\n",
    "from core.neighborhoods import NeighborhoodManager\n",
    "from core.qlearning import QLearningAgent, AdaptiveNeighborhoodSelector\n",
    "from core.shared_memory import SharedMemoryPool, Solution, ElitePool\n",
    "from core.agents import (\n",
    "    GeneticAgent, TabuAgent, SimulatedAnnealingAgent,\n",
    "    MultiAgentSystem, CollaborationMode\n",
    ")\n",
    "from visualization import (\n",
    "    plot_gantt, plot_gantt_comparison, plot_convergence,\n",
    "    plot_multi_agent_convergence, plot_q_table, plot_neighborhood_usage,\n",
    "    plot_diversity_matrix, plot_agent_contributions\n",
    ")\n",
    "\n",
    "print(\"✓ Tous les modules importés avec succès!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environnement d'Ordonnancement\n",
    "\n",
    "L'environnement modélise l'ordonnancement des patients avec:\n",
    "- 10 patients\n",
    "- 5 opérations maximum par patient\n",
    "- 6 compétences/ressources médicales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer l'environnement\n",
    "env = create_default_environment()\n",
    "\n",
    "print(f\"Configuration de l'environnement:\")\n",
    "print(f\"  - Nombre de patients: {env.num_patients}\")\n",
    "print(f\"  - Compétences: {env.skills}\")\n",
    "print(f\"  - Opérations max par patient: {env.max_ops}\")\n",
    "print(f\"  - Nombre total de tâches: {len(env.all_tasks)}\")\n",
    "\n",
    "# Solution initiale\n",
    "initial_solution = env.build_initial_solution(random_order=False)\n",
    "initial_makespan, initial_times, _ = env.evaluate(initial_solution, return_schedule=True)\n",
    "print(f\"\\nMakespan initial (ordre naïf): {initial_makespan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser le planning initial\n",
    "if initial_times:\n",
    "    fig = plot_gantt(\n",
    "        initial_times, env.skills, env.num_patients,\n",
    "        title=f\"Planning Initial (Cmax = {initial_makespan})\"\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fonctions de Voisinage\n",
    "\n",
    "5 fonctions de voisinage basées sur le diaporama:\n",
    "- **A**: Réaffectation d'une tâche à un autre médecin (MID)\n",
    "- **B**: Réaffectation de tâches successives\n",
    "- **C**: Insertion dans le même planning (MIS)\n",
    "- **D**: Échange entre différents médecins (SDMS)\n",
    "- **E**: Échange dans le même planning (SSMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Démonstration des voisinages\n",
    "nm = NeighborhoodManager()\n",
    "\n",
    "print(\"Fonctions de voisinage disponibles:\")\n",
    "for name, func in nm.neighborhoods.items():\n",
    "    print(f\"  {name}: {func.name}\")\n",
    "\n",
    "# Générer des voisins pour chaque fonction\n",
    "test_solution = env.build_initial_solution(random_order=True)\n",
    "test_makespan, _, _ = env.evaluate(test_solution)\n",
    "print(f\"\\nSolution de test: Makespan = {test_makespan}\")\n",
    "\n",
    "print(\"\\nGénération de voisins:\")\n",
    "for name in nm.neighborhood_names:\n",
    "    neighbor = nm.generate_neighbor(test_solution, name, env.skills, env.max_ops)\n",
    "    if neighbor:\n",
    "        neighbor_makespan, _, _ = env.evaluate(neighbor)\n",
    "        diff = neighbor_makespan - test_makespan\n",
    "        print(f\"  Voisinage {name}: Makespan = {neighbor_makespan} (Δ = {diff:+d})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Q-Learning pour l'Auto-Adaptation\n",
    "\n",
    "Le Q-Learning permet de sélectionner automatiquement la meilleure fonction de voisinage en fonction de l'état de la recherche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un agent Q-Learning\n",
    "q_agent = QLearningAgent(\n",
    "    states=['A', 'B', 'C', 'D', 'E'],\n",
    "    alpha=0.15,   # Taux d'apprentissage\n",
    "    gamma=0.9,    # Facteur d'actualisation\n",
    "    epsilon=0.5,  # Exploration initiale\n",
    "    epsilon_decay=0.99\n",
    ")\n",
    "\n",
    "print(\"Paramètres Q-Learning:\")\n",
    "print(f\"  α (learning rate): {q_agent.alpha}\")\n",
    "print(f\"  γ (discount factor): {q_agent.gamma}\")\n",
    "print(f\"  ε (exploration): {q_agent.epsilon}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation d'apprentissage\n",
    "reward_means = {'A': 0.8, 'B': 0.5, 'C': 0.6, 'D': 0.3, 'E': 0.4}\n",
    "epsilon_history = []\n",
    "\n",
    "for episode in range(300):\n",
    "    action = q_agent.select_action()\n",
    "    reward = random.gauss(reward_means[action], 0.2)\n",
    "    q_agent.update(action, reward)\n",
    "    q_agent.decay_epsilon()\n",
    "    epsilon_history.append(q_agent.epsilon)\n",
    "\n",
    "# Visualiser la table Q\n",
    "q_table = q_agent.get_q_table_formatted()\n",
    "fig = plot_q_table(q_table, title=\"Table Q après apprentissage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Espace Mémoire Partagé (EMP) avec Diversité\n",
    "\n",
    "L'EMP stocke les bonnes solutions trouvées tout en maintenant leur diversité. La distance entre solutions est mesurée par le nombre de différences dans les plannings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer l'EMP\n",
    "emp = SharedMemoryPool(\n",
    "    max_size=15,\n",
    "    min_distance=3,      # R: distance minimale\n",
    "    diversity_threshold=0.4  # DT: seuil de diversité\n",
    ")\n",
    "\n",
    "print(\"Paramètres EMP:\")\n",
    "print(f\"  Taille max: {emp.max_size}\")\n",
    "print(f\"  Distance minimale (R): {emp.min_distance}\")\n",
    "print(f\"  Seuil de diversité (DT): {emp.diversity_threshold}\")\n",
    "\n",
    "# Remplir l'EMP\n",
    "for i in range(40):\n",
    "    sol = env.build_initial_solution(random_order=True)\n",
    "    fitness, _, _ = env.evaluate(sol)\n",
    "    solution = Solution(sequences=sol, fitness=fitness, agent_id=f\"test_{i}\")\n",
    "    emp.insert(solution, iteration=i)\n",
    "\n",
    "stats = emp.get_statistics()\n",
    "print(f\"\\nStatistiques EMP:\")\n",
    "print(f\"  Solutions stockées: {stats['size']}/{stats['max_size']}\")\n",
    "print(f\"  Meilleure fitness: {stats['best_fitness']}\")\n",
    "print(f\"  Insertions: {stats['insertions']}\")\n",
    "print(f\"  Rejets (doublons): {stats['rejections_duplicate']}\")\n",
    "print(f\"  Rejets (diversité): {stats['rejections_diversity']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de diversité\n",
    "if len(emp.solutions) > 1:\n",
    "    div_matrix = emp.get_diversity_matrix()\n",
    "    fig = plot_diversity_matrix(div_matrix, title=\"Matrice de Diversité de l'EMP\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Système Multi-Agents - Mode AMIS\n",
    "\n",
    "En mode Amis, les agents partagent leurs solutions complètes via l'EMP. Cela permet une collaboration efficace et une diversification de la recherche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le système multi-agents en mode Amis\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "mas_friends = MultiAgentSystem(env, mode=CollaborationMode.FRIENDS, use_qlearning=True)\n",
    "\n",
    "# Ajouter des agents de différents types\n",
    "mas_friends.add_agent('genetic', 'AG_1', population_size=12, mutation_rate=0.15)\n",
    "mas_friends.add_agent('genetic', 'AG_2', population_size=10)\n",
    "mas_friends.add_agent('tabu', 'Tabu_1', tabu_tenure=12)\n",
    "mas_friends.add_agent('sa', 'RS_1', initial_temp=80, cooling_rate=0.99)\n",
    "mas_friends.add_agent('sa', 'RS_2', initial_temp=100)\n",
    "\n",
    "print(f\"Mode: {mas_friends.mode}\")\n",
    "print(f\"Agents: {list(mas_friends.agents.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exécuter l'optimisation\n",
    "print(\"Optimisation collaborative en cours...\")\n",
    "best_solution_friends = mas_friends.run(n_iterations=150, verbose=True)\n",
    "\n",
    "stats_friends = mas_friends.get_statistics()\n",
    "print(f\"\\nMeilleur Makespan trouvé: {stats_friends['global_best_fitness']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser la convergence\n",
    "fig = plot_convergence(\n",
    "    stats_friends['fitness_history'],\n",
    "    title=\"Convergence Multi-Agents (Mode Amis)\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Contributions des agents\n",
    "fig = plot_agent_contributions(\n",
    "    mas_friends.agent_contributions,\n",
    "    title=\"Contributions des Agents (Mode Amis)\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser le planning optimisé\n",
    "if best_solution_friends:\n",
    "    final_makespan, final_times, _ = env.evaluate(\n",
    "        best_solution_friends.sequences, return_schedule=True\n",
    "    )\n",
    "    \n",
    "    fig = plot_gantt_comparison(\n",
    "        initial_times, final_times,\n",
    "        env.skills, env.num_patients,\n",
    "        initial_makespan, final_makespan,\n",
    "        title=\"Comparaison Avant/Après Optimisation (Mode Amis)\"\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Système Multi-Agents - Mode ENNEMIS\n",
    "\n",
    "En mode Ennemis, les agents ne partagent que leurs valeurs de fitness. Un agent ne travaille que si une meilleure solution globale existe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le système en mode Ennemis\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "mas_enemies = MultiAgentSystem(env, mode=CollaborationMode.ENEMIES, use_qlearning=True)\n",
    "\n",
    "mas_enemies.add_agent('genetic', 'AG_Enemy')\n",
    "mas_enemies.add_agent('tabu', 'Tabu_Enemy')\n",
    "mas_enemies.add_agent('sa', 'RS_Enemy')\n",
    "\n",
    "print(f\"Mode: {mas_enemies.mode}\")\n",
    "print(\"\\nEn mode Ennemis, les agents sont en compétition.\")\n",
    "print(\"Un agent ne travaille que si une meilleure solution existe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exécuter l'optimisation\n",
    "print(\"\\nOptimisation compétitive en cours...\")\n",
    "best_solution_enemies = mas_enemies.run(n_iterations=150, verbose=True)\n",
    "\n",
    "stats_enemies = mas_enemies.get_statistics()\n",
    "print(f\"\\nMeilleur Makespan trouvé: {stats_enemies['global_best_fitness']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparer les convergences\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(stats_friends['fitness_history'], 'b-', linewidth=1.5)\n",
    "ax1.set_title(f\"Mode AMIS\\nMeilleur: {stats_friends['global_best_fitness']}\")\n",
    "ax1.set_xlabel(\"Itération\")\n",
    "ax1.set_ylabel(\"Makespan\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(stats_enemies['fitness_history'], 'r-', linewidth=1.5)\n",
    "ax2.set_title(f\"Mode ENNEMIS\\nMeilleur: {stats_enemies['global_best_fitness']}\")\n",
    "ax2.set_xlabel(\"Itération\")\n",
    "ax2.set_ylabel(\"Makespan\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Comparaison des Modes de Collaboration\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparaison Finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau de comparaison\n",
    "print(\"=\"*60)\n",
    "print(\"       COMPARAISON DES MODES DE COLLABORATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Métrique':<35} {'AMIS':>12} {'ENNEMIS':>12}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Meilleur Makespan':<35} {stats_friends['global_best_fitness']:>12} {stats_enemies['global_best_fitness']:>12}\")\n",
    "print(f\"{'Taille finale EMP':<35} {stats_friends['emp_stats']['size']:>12} {stats_enemies['emp_stats']['size']:>12}\")\n",
    "print(f\"{'Insertions EMP':<35} {stats_friends['emp_stats']['insertions']:>12} {stats_enemies['emp_stats']['insertions']:>12}\")\n",
    "\n",
    "contrib_friends = sum(a['contributions'] for a in stats_friends['agent_stats'].values())\n",
    "contrib_enemies = sum(a['contributions'] for a in stats_enemies['agent_stats'].values())\n",
    "print(f\"{'Contributions totales':<35} {contrib_friends:>12} {contrib_enemies:>12}\")\n",
    "\n",
    "improvement_friends = (initial_makespan - stats_friends['global_best_fitness']) / initial_makespan * 100\n",
    "improvement_enemies = (initial_makespan - stats_enemies['global_best_fitness']) / initial_makespan * 100\n",
    "print(f\"{'Amélioration vs initial (%)':<35} {improvement_friends:>11.1f}% {improvement_enemies:>11.1f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planning final optimisé\n",
    "if stats_friends['global_best_fitness'] <= stats_enemies['global_best_fitness']:\n",
    "    best_overall = best_solution_friends\n",
    "    best_mode = \"AMIS\"\n",
    "else:\n",
    "    best_overall = best_solution_enemies\n",
    "    best_mode = \"ENNEMIS\"\n",
    "\n",
    "print(f\"\\nMeilleure solution globale trouvée par le mode {best_mode}\")\n",
    "\n",
    "final_makespan, final_times, _ = env.evaluate(best_overall.sequences, return_schedule=True)\n",
    "\n",
    "fig = plot_gantt(\n",
    "    final_times, env.skills, env.num_patients,\n",
    "    title=f\"Planning Optimisé Final (Mode {best_mode}, Cmax = {final_makespan})\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Ce système multi-agents démontre comment combiner:\n",
    "\n",
    "1. **Plusieurs métaheuristiques** (AG, Tabou, Recuit) pour diversifier la recherche\n",
    "2. **Deux modes de collaboration** (Amis/Ennemis) pour différentes stratégies\n",
    "3. **L'auto-adaptation via Q-Learning** pour choisir intelligemment les voisinages\n",
    "4. **Le contrôle de diversité** dans l'EMP pour éviter la convergence prématurée\n",
    "\n",
    "Les résultats montrent que:\n",
    "- Le mode **Amis** favorise le partage d'informations et la convergence rapide\n",
    "- Le mode **Ennemis** encourage la compétition et peut trouver des solutions alternatives\n",
    "- Le **Q-Learning** permet d'adapter automatiquement la stratégie de recherche"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
